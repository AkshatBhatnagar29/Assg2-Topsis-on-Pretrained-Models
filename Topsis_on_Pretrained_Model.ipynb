{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpYQy9BTcqpEnWu0KWY/HP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkshatBhatnagar29/Assg2-Topsis-on-Pretrained-Models/blob/main/Topsis_on_Pretrained_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raJ_tR8Ku-Jb"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers scikit-learn pandas numpy scipy torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Loading STS Benchmark dataset...\")\n",
        "sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\n",
        "\n",
        "\n",
        "models_list = [\n",
        "    'sentence-transformers/all-MiniLM-L6-v2',\n",
        "    'sentence-transformers/all-mpnet-base-v2',\n",
        "    'sentence-transformers/paraphrase-MiniLM-L6-v2',\n",
        "    'sentence-transformers/multi-qa-MiniLM-L6-cos-v1',\n",
        "    'sentence-transformers/all-distilroberta-v1'\n",
        "]"
      ],
      "metadata": {
        "id": "bWlaqRAfvRyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(sts)\n",
        "\n",
        "df[\"label\"] = df[\"label\"] / 5.0\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "qkOCyveCvN3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for model_id in models_list:\n",
        "    model_name = model_id.split(\"/\")[-1]\n",
        "    print(f\"\\nEvaluating model: {model_name}\")\n",
        "\n",
        "    model = SentenceTransformer(model_id)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Batch encode sentences\n",
        "    sentences1 = df[\"sentence1\"].tolist()\n",
        "    sentences2 = df[\"sentence2\"].tolist()\n",
        "\n",
        "    embeddings1 = model.encode(\n",
        "        sentences1,\n",
        "        batch_size=32,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    embeddings2 = model.encode(\n",
        "        sentences2,\n",
        "        batch_size=32,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    # Cosine similarity (vectorized)\n",
        "    model_scores = np.sum(embeddings1 * embeddings2, axis=1) / (\n",
        "        np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n",
        "    )\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    avg_time = elapsed_time / len(df)\n",
        "\n",
        "    # Correlation with human scores\n",
        "    pearson = pearsonr(model_scores, df[\"label\"])[0]\n",
        "    spearman = spearmanr(model_scores, df[\"label\"])[0]\n",
        "\n",
        "    embedding_dim = model.get_sentence_embedding_dimension()\n",
        "\n",
        "    results.append([\n",
        "        model_name,\n",
        "        pearson,\n",
        "        spearman,\n",
        "        avg_time,\n",
        "        embedding_dim\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "XtGaGtZrvNqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_matrix = pd.DataFrame(\n",
        "    results,\n",
        "    columns=[\"Model\", \"Pearson\", \"Spearman\", \"Avg_Time\", \"Embedding_Dim\"]\n",
        ")\n",
        "\n",
        "decision_matrix\n"
      ],
      "metadata": {
        "id": "7VR3spLnvmR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_sizes = {\n",
        "    \"all-MiniLM-L6-v2\": 90,\n",
        "    \"all-mpnet-base-v2\": 420,\n",
        "    \"paraphrase-MiniLM-L6-v2\": 90,\n",
        "    \"multi-qa-MiniLM-L6-cos-v1\": 90,\n",
        "    \"all-distilroberta-v1\": 305\n",
        "}\n",
        "\n",
        "decision_matrix[\"Model_Size_MB\"] = decision_matrix[\"Model\"].map(model_sizes)\n",
        "decision_matrix\n"
      ],
      "metadata": {
        "id": "GMmXo78iv8Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topsis_data = decision_matrix.drop(columns=[\"Model\"]).values\n",
        "\n",
        "weights = np.array([0.3, 0.3, 0.15, 0.1, 0.15])\n",
        "\n",
        "impacts = ['+', '+', '-', '-', '-']\n"
      ],
      "metadata": {
        "id": "W24A9wobxyoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm = topsis_data / np.sqrt((topsis_data ** 2).sum(axis=0))\n",
        "weighted = norm * weights\n"
      ],
      "metadata": {
        "id": "Rt4g-VX2yYNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ideal_best = []\n",
        "ideal_worst = []\n",
        "\n",
        "for i, imp in enumerate(impacts):\n",
        "    if imp == '+':\n",
        "        ideal_best.append(weighted[:, i].max())\n",
        "        ideal_worst.append(weighted[:, i].min())\n",
        "    else:\n",
        "        ideal_best.append(weighted[:, i].min())\n",
        "        ideal_worst.append(weighted[:, i].max())\n",
        "\n",
        "ideal_best = np.array(ideal_best)\n",
        "ideal_worst = np.array(ideal_worst)\n"
      ],
      "metadata": {
        "id": "ReaDHpaHyaso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dist_best = np.sqrt(((weighted - ideal_best) ** 2).sum(axis=1))\n",
        "dist_worst = np.sqrt(((weighted - ideal_worst) ** 2).sum(axis=1))\n",
        "\n",
        "topsis_score = dist_worst / (dist_best + dist_worst)\n",
        "\n",
        "decision_matrix[\"TOPSIS_Score\"] = topsis_score\n",
        "decision_matrix[\"Rank\"] = decision_matrix[\"TOPSIS_Score\"].rank(ascending=False)\n",
        "\n",
        "decision_matrix.sort_values(\"Rank\")\n"
      ],
      "metadata": {
        "id": "yMqXgvSTydFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = decision_matrix.sort_values(\"Rank\").iloc[0]\n",
        "best_model\n"
      ],
      "metadata": {
        "id": "y2r9_KudyhAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Create the dataset based on your result table\n",
        "data = {\n",
        "    'Model': [\n",
        "        'paraphrase-MiniLM-L6-v2',\n",
        "        'all-MiniLM-L6-v2',\n",
        "        'multi-qa-MiniLM-L6-cos-v1',\n",
        "        'all-distilroberta-v1',\n",
        "        'all-mpnet-base-v2'\n",
        "    ],\n",
        "    'TOPSIS_Score': [0.980174, 0.974779, 0.889983, 0.495963, 0.107538]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2. Sort data to have the highest score on the left\n",
        "df = df.sort_values(by='TOPSIS_Score', ascending=False)\n",
        "\n",
        "# 3. Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Create the bar plot\n",
        "# We use a color palette that highlights the top ranks\n",
        "barplot = sns.barplot(\n",
        "    x='TOPSIS_Score',\n",
        "    y='Model',\n",
        "    data=df,\n",
        "    palette='viridis',\n",
        "    hue='Model',\n",
        "    legend=False\n",
        ")\n",
        "\n",
        "# 4. Add labels and title\n",
        "plt.title('TOPSIS Ranking of Pretrained Models', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('TOPSIS Score', fontsize=12)\n",
        "plt.ylabel('Model Name', fontsize=12)\n",
        "plt.xlim(0, 1.1)  # Set x-axis limit slightly above 1 for spacing\n",
        "\n",
        "# 5. Add the score text at the end of each bar for clarity\n",
        "for i, v in enumerate(df['TOPSIS_Score']):\n",
        "    barplot.text(v + 0.01, i, f\"{v:.4f}\", va='center', fontweight='bold', color='black')\n",
        "\n",
        "# 6. Save or Show the plot\n",
        "plt.tight_layout()\n",
        "plt.savefig('topsis_ranking_graph.png', dpi=300) # Saves high-res image\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YG4qj11X0Mvq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}